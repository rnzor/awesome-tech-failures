{"id":"microsoft-tay-2016","title":"Microsoft Tay — Prompt Injection & Adversarial Manipulation","year":2016,"evidence_type":"direct_incident","category":"ai-slop","cause":"ai","stage":"growth","impact":["trust"],"summary":"Microsoft launched an AI chatbot designed to learn from Twitter users. Within 24 hours, adversarial users manipulated it into generating racist, antisemitic, and inflammatory content.","root_cause":"No adversarial input filtering, no content safeguards, trusted users would train it well","lessons":["Never expose learning systems to unmoderated public input without safeguards","Assume adversarial actors will test every boundary","Trust users will teach good things is not a valid architectural assumption"],"patterns":["misconfigured-trust-boundaries"],"tags":["prompt-injection","adversarial","no-guardrails","brand-damage","twitter"],"sources":[{"title":"BBC News","url":"https://www.bbc.com/news/technology-35902104","kind":"primary"}]}
{"id":"ai-agent-deletes-production-2023","title":"AI Agent Deletes Production Data","year":2023,"evidence_type":"repeated_pattern","category":"ai-slop","cause":"automation","stage":"scale","impact":["data-loss"],"summary":"An LLM-powered agent with write access executed destructive commands during an ambiguous task, resulting in partial production data loss.","root_cause":"Blind trust in agent autonomy, no guardrails on destructive operations, write access without reversible action requirements","lessons":["AI agents are junior interns with superpowers — they need oversight","Never grant write access without reversible actions and kill switches","All destructive operations require explicit human approval"],"patterns":["automation-without-reversal","blind-trust-in-ai-output"],"tags":["no-guardrails","blind-trust","agent-failure","write-access","autonomous-systems"],"sources":[{"title":"Enterprise AI News","url":"https://www.enterpriseai.news/2023/11/when-ai-agents-go-wrong-the-growing-pain-of-autonomous-systems/","kind":"primary"}]}
{"id":"runaway-ai-agents-cost-2023","title":"Runaway AI Agents Causing Massive Costs","year":2023,"evidence_type":"direct_incident","category":"ai-slop","cause":"automation","stage":"growth","impact":["money"],"summary":"Autonomous LLM agents entered infinite loops or continuously called expensive APIs, generating thousands of dollars in token costs within hours.","root_cause":"No token or cost limits on agent actions, recursive self-improvement loops, lack of execution budgets","lessons":["Always implement cost limits on autonomous agents","Set execution budgets before letting agents run unattended","Monitor token usage in real-time with hard limits"],"patterns":["automation-without-reversal","overconfidence-from-past-success"],"tags":["runaway-costs","token-explosion","agent-loops","no-limits"],"sources":[{"title":"OpenAI Community","url":"https://community.openai.com/t/gpt-agent-burned-7000-overnight/4821","kind":"primary"}]}
{"id":"ai-generated-code-breaks-production-2023","title":"AI-Generated Code Shipped Without Review","year":2023,"evidence_type":"repeated_pattern","category":"ai-slop","cause":"ai","stage":"growth","impact":["users"],"summary":"Engineering teams merged AI-generated code without adequate review, leading to production bugs, security vulnerabilities, and maintainability issues.","root_cause":"Time pressure to deliver, overconfidence in AI code quality, belief that AI knows more than me","lessons":["AI-generated code requires at least as much review as human code","Never ship AI code that you don't understand","AI is a generator, not a validator — outputs need human validation"],"patterns":["blind-trust-in-ai-output","overconfidence-from-past-success"],"tags":["ai-generated-code","no-review","production-bug","copilot"],"sources":[{"title":"Stack Overflow","url":"https://stackoverflow.blog/2023/11/16/the-risks-of-ai-code-generators/","kind":"primary"}]}
{"id":"ai-seo-content-collapse-2023","title":"AI-Generated SEO Content Collapse","year":2023,"evidence_type":"direct_incident","category":"ai-slop","cause":"ai","stage":"growth","impact":["money"],"summary":"Large-scale AI content replaced human-written pages without editorial review, triggering search engine ranking drops across the site.","root_cause":"Low-quality AI output deployed without human review, volume prioritized over quality, no content quality gates","lessons":["Search engines reward usefulness, not volume","AI amplifies strategy — good or bad","Editorial review is mandatory for AI-generated content"],"patterns":["blind-trust-in-ai-output","decision-making-by-proxy"],"tags":["seo-collapse","low-quality-content","automation-without-review","content-strategy"],"sources":[{"title":"Ahrefs","url":"https://www.ahrefs.com/blog/ai-content-seo/","kind":"primary"}]}
{"id":"prompt-injection-data-exposure-2023","title":"Prompt Injection Leading to Data Exposure","year":2023,"evidence_type":"direct_incident","category":"ai-slop","cause":"ai","stage":"scale","impact":["data-loss"],"summary":"Adversarial prompt injection attacks tricked AI systems into revealing sensitive information, bypassing safety measures and outputting private data.","root_cause":"Prompt injection as a novel attack vector, AI systems trusting user input too readily, lack of input sanitization","lessons":["Prompt injection is the SQL injection of the AI era","Validate and sanitize all inputs, including hidden prompt fields","Separate untrusted input from system prompts"],"patterns":["misconfigured-trust-boundaries","automation-without-reversal"],"tags":["prompt-injection","data-exposure","security","adversarial-input"],"sources":[{"title":"OWASP","url":"https://owasp.org/www-project-top-10-for-large-language-model-applications/","kind":"primary"}]}
{"id":"ai-summaries-replacing-understanding-2023","title":"AI Summaries Replacing Understanding","year":2023,"evidence_type":"repeated_pattern","category":"ai-slop","cause":"ai","stage":"scale","impact":["morale"],"summary":"Teams relied on AI-generated summaries instead of reading source data, leading to cascading bad strategic decisions based on incomplete understanding.","root_cause":"Delegating thinking not just toil, AI used to compress data but output treated as authoritative","lessons":["AI should compress data, not replace judgment","Leaders must still read the source for critical decisions","Summary is a starting point, not the endpoint of reasoning"],"patterns":["decision-making-by-proxy","blind-trust-in-ai-output"],"tags":["ai-summaries","decision-degradation","blind-trust","automation-bias"],"sources":[{"title":"McKinsey","url":"https://www.mckinsey.com/capabilities/strategy-and-corporate-finance/our-insights/the-risk-of-relying-on-ai-summaries","kind":"primary"}]}
{"id":"vibe-coding-unmaintainable-2024","title":"Vibe Coding Unmaintainable Systems","year":2024,"evidence_type":"repeated_pattern","category":"ai-slop","cause":"ai","stage":"early","impact":["morale"],"summary":"Teams used AI coding tools extensively to build entire features or backends, only to discover the code was unmaintainable, undocumented, and impossible for humans to extend.","root_cause":"Vibe coding — generating code without understanding it, no documentation generated, no architectural oversight","lessons":["Code you don't understand is technical debt from day one","AI can generate code, but humans must own architecture","Require documentation and code review for AI-generated code"],"patterns":["blind-trust-in-ai-output","overconfidence-from-past-success"],"tags":["vibe-coding","technical-debt","ai-generated-codebase","unmaintainable"],"sources":[{"title":"Indie Hackers","url":"https://www.indiehackers.com/post/what-happened-when-we-let-ai-write-our-whole-backend","kind":"primary"}]}
{"id":"over-automated-customer-support-2023","title":"Over-Automated Customer Support Destroying Trust","year":2023,"evidence_type":"direct_incident","category":"ai-slop","cause":"automation","stage":"scale","impact":["trust"],"summary":"Companies deployed AI customer support chatbots without adequate human escalation paths, leading to misinformation, frustrated customers, and trust damage.","root_cause":"Cost-cutting focus over customer experience, no clear escalation paths, chatbots empowered to make promises they couldn't keep","lessons":["Customer trust is hard to earn, easy to lose","AI chatbots need clear escalation paths to humans","Never let AI make promises it can't verify"],"patterns":["automation-without-reversal","decision-making-by-proxy"],"tags":["customer-support","chatbot-failure","no-escalation","trust-destruction"],"sources":[{"title":"CBC News","url":"https://www.cbc.ca/news/canada/air-canada-ai-chatbot-misinformation-1.7139752","kind":"primary"}]}
{"id":"github-database-outage-2018","title":"GitHub Database Outage","year":2018,"evidence_type":"direct_incident","category":"outage","cause":"architecture","stage":"scale","impact":["users"],"summary":"A routine maintenance operation triggered a large-scale MySQL replication failure, causing hours of downtime across GitHub's platform.","root_cause":"Overconfidence in replication infrastructure, insufficient understanding of failure modes, restore paths never tested","lessons":["Backups are useless if restore paths are not tested","We have done this before is not a safety guarantee","Understand failure modes before they happen"],"patterns":["hidden-single-point-of-failure","overconfidence-from-past-success"],"tags":["hidden-dependency","rollback-failure","observability-gap","mysql","replication"],"sources":[{"title":"GitHub Blog","url":"https://github.blog/2018-10-30-october-incident-report/","kind":"primary"}]}
{"id":"aws-s3-us-east-1-2017","title":"AWS S3 us-east-1 Outage","year":2017,"evidence_type":"direct_incident","category":"outage","cause":"human-error","stage":"scale","impact":["users"],"summary":"A typo during debugging commands took down S3 in the us-east-1 region, cascading into major internet outages for thousands of services.","root_cause":"Human error during debugging, insufficient blast-radius controls, region-level dependencies as silent killers","lessons":["Highly available does not mean invulnerable","Region-level dependencies are silent killers","Debugging in production requires extreme caution"],"patterns":["hidden-single-point-of-failure","automation-without-reversal"],"tags":["blast-radius","control-plane","region-dependency","s3","typo"],"sources":[{"title":"AWS","url":"https://aws.amazon.com/message/41926/","kind":"primary"}]}
{"id":"knight-capital-2012","title":"Knight Capital Trading Loss","year":2012,"evidence_type":"direct_incident","category":"outage","cause":"automation","stage":"scale","impact":["money"],"summary":"A faulty deployment activated dead code paths in Knight Capital's trading system, triggering uncontrolled trading that lost approximately $440M in under an hour.","root_cause":"Broken deployment process, dead code left in production, no kill switches, automation without human oversight","lessons":["Automation without kill switches is lethal","Old code is still production code — audit and remove dead paths","Every automated system needs a tested emergency stop"],"patterns":["automation-without-reversal"],"tags":["no-rollback","deployment-failure","trading-automation","dead-code","kill-switch"],"sources":[{"title":"SEC","url":"https://www.sec.gov/spotlight/equity-markets-structure-committee/knight-capital-report.pdf","kind":"primary"}]}
{"id":"cloudflare-regex-catastrophe-2019","title":"Cloudflare Regex Catastrophe","year":2019,"evidence_type":"direct_incident","category":"outage","cause":"architecture","stage":"scale","impact":["trust"],"summary":"A single regex rule deployed to Cloudflare's WAF caused CPU exhaustion across all HTTP/HTTPS servers worldwide, resulting in 30+ minutes of 502 errors.","root_cause":"The regex caused catastrophic backtracking, rule not tested for CPU impact, safety mechanism removed by mistake","lessons":["Test regex performance before deployment","Any change to safety mechanisms requires review and rollback plan","Single points of failure in safety systems are catastrophic"],"patterns":["overconfidence-from-past-success"],"tags":["regex","backtracking","cpu-exhaustion","waf","global-impact","cascade"],"sources":[{"title":"Cloudflare Blog","url":"https://blog.cloudflare.com/details-of-the-cloudflare-outage-on-july-2-2019/","kind":"primary"}]}
{"id":"capital-one-breach-2019","title":"Capital One Breach","year":2019,"evidence_type":"direct_incident","category":"security","cause":"human-error","stage":"scale","impact":["trust"],"summary":"An attacker exploited a misconfigured cloud firewall combined with SSRF to access sensitive customer data stored in AWS, exposing 100M records.","root_cause":"Misconfigured IAM roles with over-privileged access, internal services assumed trusted without validation","lessons":["Cloud security failures are often configuration, not complex exploits","Assume internal services will be abused — apply zero-trust principles","IAM permissions should follow least-privilege"],"patterns":["misconfigured-trust-boundaries"],"tags":["misconfiguration","iam","ssrf","cloud-security","over-privileged-access"],"sources":[{"title":"DOJ","url":"https://www.justice.gov/opa/press-release/file/1244101/download","kind":"primary"}]}
{"id":"equifax-breach-2017","title":"Equifax Breach","year":2017,"evidence_type":"direct_incident","category":"security","cause":"human-error","stage":"scale","impact":["data-loss"],"summary":"Attackers exploited a known Apache Struts vulnerability that had a patch available for 2 months, exposing 147.9 million Americans' data.","root_cause":"Failure to patch a critical vulnerability despite knowing about it, no network segmentation, plaintext credentials","lessons":["Patch critical vulnerabilities within hours/days, not weeks/months","Network segmentation limits blast radius of initial compromise","Store credentials securely"],"patterns":["patching-debt"],"tags":["unpatched","apache-struts","credential-lateral-movement","no-segmentation","regulatory-fine"],"sources":[{"title":"BreachSense","url":"https://www.breachsense.com/blog/equifax-data-breach/","kind":"primary"}]}
